{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Machine Learning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***Linear Regression Regularized***"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "train_data = pd.read_csv(\"../data/processed/clean_train.csv\")\n",
                "test_data = pd.read_csv(\"../data/processed/clean_test.csv\")\n",
                "\n",
                "X_train = train_data.drop([\"diabetes_number\"], axis = 1)\n",
                "y_train = train_data[\"diabetes_number\"]\n",
                "X_test = test_data.drop([\"diabetes_number\"], axis = 1)\n",
                "y_test = test_data[\"diabetes_number\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
                        "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
                        "\n",
                        "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
                        "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
                        "Please also refer to the documentation for alternative solver options:\n",
                        "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
                        "  n_iter_i = _check_optimize_result(\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
                        ],
                        "text/plain": [
                            "LogisticRegression()"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "model = LogisticRegression()\n",
                "model.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Intercep (a): [-0.29831275 -0.29864405 -0.29699163 ... -2.28321979 -2.06773185\n",
                        " -1.83290569]\n",
                        "Coefficients: [[-0.07064728 -0.06596914 -0.07228268 ... -0.10724544 -0.11779368\n",
                        "  -0.08281012]\n",
                        " [-0.07079265 -0.06662841 -0.07293153 ... -0.10697491 -0.11779601\n",
                        "  -0.0824732 ]\n",
                        " [-0.07049045 -0.0659779  -0.07230404 ... -0.10682884 -0.1175275\n",
                        "  -0.08249867]\n",
                        " ...\n",
                        " [ 0.18749449  0.13673308  0.09610727 ...  0.12590013  0.21012718\n",
                        "   0.28810445]\n",
                        " [ 0.23291953  0.40323466  0.26524437 ...  0.1780084   0.05016247\n",
                        "   0.1499571 ]\n",
                        " [ 0.20856109  0.10580991  0.1214792  ...  0.0990296   0.09587355\n",
                        "   0.24653583]]\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Intercep (a): {model.intercept_}\")\n",
                "print(f\"Coefficients: {model.coef_}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([   547,  22006,    547,   7922,  21283,    547,    547,    547,\n",
                            "          547,    547,    547,    547,   7922,    547,   7922,    547,\n",
                            "       137266,    547,    547,    547,    547,    547,    547,    547,\n",
                            "        91707,   2687,    547,    547,    547,    547,    547,    547,\n",
                            "          547,    547,   7922,    547,    547,    547,    547,    547,\n",
                            "          547,  21283,    547,    547,   2687,    547,    547,   8456,\n",
                            "          547,    547,  47754,    547,    547,    547,  21283,    547,\n",
                            "        83461,    547,   2687,    547,  22006,    547,    547,  22006,\n",
                            "          547,   2687,    547,    547,    547,  22006,    547,    547,\n",
                            "          547,    547,    547,   2687,  47754,    547,    547,    547,\n",
                            "          547,  10829,  22002,    547,    547,    547,  58354,    547,\n",
                            "          547,  10829,    547,    547,    547,    547,    547,    547,\n",
                            "          547,    547,  21283,    547, 101883,  21283,    547,    547,\n",
                            "          547,    547,    547,    547,  21283,  21283,    547,   8456,\n",
                            "        40587,   2687,    547,  58354,    547,    547,  10829,    547,\n",
                            "          547,    547,    547,   2687,    547,    547,   7922,    547,\n",
                            "        21283,   2687,    547,    547,    547,    547,   2687,    547,\n",
                            "          547,  22006,   2687,    547,    547,    547,  83072,    547,\n",
                            "          547,    547,    547,    547,   8456,    547,    547,  61275,\n",
                            "          547,   2687,    547,    547,    547,    547,   7922,    547,\n",
                            "          547,    547,    547,    547,    547,   2687,    547,   7922,\n",
                            "        47754,    547,    547,  21283,    547,    547,   2687,    547,\n",
                            "          547,   7922,   2687,    547,    547,    547,    547,    547,\n",
                            "          547,    547,    547,    547,    547,    547,  10829,    547,\n",
                            "          547,    547,    547,    547,    547,    547,    547,    547,\n",
                            "        21283,   8456, 143158,    547,   2687,    547,    547,    547,\n",
                            "          547,    547,    547,    547,    547,    547,    547,  22002,\n",
                            "          547,    547,    547,  22006,    547,    547,    547,    547,\n",
                            "        21283,    547,  21283,    547,    547,    547,    547,    547,\n",
                            "          547,    547,    547,    547,    547,    547,    547,  21283,\n",
                            "         2687,    547,    547,    547,    547,    547,   2687,    547,\n",
                            "        47754,    547,    547,    547,    547,    547,  80943,    547,\n",
                            "          547,   7922,    547,   2687,    547,    547,    547,    547,\n",
                            "          547,   2687,    547,    547,    547,    547,   2687,    547,\n",
                            "         7922,    547,    547,    547,   2687,   2687,    547,    547,\n",
                            "        67471,  21283,    547,    547,    547,    547,    547,    547,\n",
                            "          547,  21283,    547,   7922,    547,    547,  21283,    547,\n",
                            "        42445,  22006,    547,    547,   7922,    547,    547,    547,\n",
                            "          547,    547,    547,    547,  21283,   2687,    547,    547,\n",
                            "         2687,    547,    547,  21283,    547,    547,    547,    547,\n",
                            "          547,    547,    547,    547,    547,  21283,    547,    547,\n",
                            "          547,  21283,    547,   8537,  19838,    547,    547,    547,\n",
                            "         2687,   2687,   7922,    547,    547,    547,    547,    547,\n",
                            "         2687,    547,    547,    547,    547,    547,    547,   2687,\n",
                            "          547,   7922,   2687,    547,    547,    547,    547,   2687,\n",
                            "          547,    547,    547,    547,    547,    547,  22006,  22006,\n",
                            "          547,    547,    547,   8456,    547,    547,  47754,   8456,\n",
                            "          547,    547,   7922,    547,    547,   2687,  61120,  21283,\n",
                            "          547,    547,    547,    547,    547,    547,   2687,    547,\n",
                            "          547,    547,    547,    547,    547,    547,    547,    547,\n",
                            "          547,    547,   2687,    547,  50762,    547,    547,    547,\n",
                            "          547,   8456,    547,    547,    547,    547,    547, 151431,\n",
                            "          547,   2687,    547,   2687,    547,  21283,    547,   2687,\n",
                            "          547,  22006,    547,  21283,    547,  80680,    547,    547,\n",
                            "          547,    547,   2687,    547,    547,    547,  77645,  67471,\n",
                            "          547,    547,    547,  48127,    547,   8537,    547,    547,\n",
                            "          547,    547,    547,    547,    547,    547,    547,  21283,\n",
                            "          547,    547,   7922,    547,  21283,    547,    547,    547,\n",
                            "          547,    547,    547,    547,   2687,    547,    547,  10829,\n",
                            "          547,    547,    547,    547,    547,    547,    547,    547,\n",
                            "         2687,    547,    547,    547,  29825,    547,   2687,    547,\n",
                            "         7922,    547,  21283,   8456,    547,    547,    547,  61120,\n",
                            "          547,    547,  87571,    547,    547,  61478,   7922,    547,\n",
                            "          547,    547,    547,   2687,   2687,    547,    547,    547,\n",
                            "          547,   2687,    547,   2687,   2687,   2687,   7922,    547,\n",
                            "          547,    547,    547,    547,  42445,    547,   7922,  21283,\n",
                            "          547,   2687,    547,    547,    547,    547,   2687,    547,\n",
                            "          547,    547,  61120,  43261,   2687,    547,   2687,  73191,\n",
                            "          547,  22006,    547,   2687,   2687,    547,    547,   2687,\n",
                            "          547,    547,   2687,    547,   2687,    547,  10829,  75829,\n",
                            "          547,    547,  79326,    547,    547,   8537,  78504,   8456,\n",
                            "          547,    547,   2687,    547,    547,    547,    547,  21283,\n",
                            "          547,    547,    547,   2687,    547,    547,    547,    547,\n",
                            "          547,    547,    547,   2687,    547,   7922,  10829,    547,\n",
                            "          547,  21283,  83461,    547,    547,    547,   2687,    547,\n",
                            "          547,   7922,    547,  10829,    547,    547,  21283,    547,\n",
                            "         8456,   2687,    547,    547,  22006,    547,    547,    547,\n",
                            "          547,  22006,    547,    547,    547,  21283,    547,    547,\n",
                            "          547,    547,   8537,    547])"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "y_pred = model.predict(X_test)\n",
                "y_pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MSE: 84351582.17197452\n",
                        "R2 Score: 0.709130287138983\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\n",
                "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Coefficients: [ 1.69229020e+04 -4.33774001e+03 -2.38997568e+03 -8.40095694e+01\n",
                        " -1.44420116e+03  1.38700636e+04  5.97999674e+03 -6.57737533e+01\n",
                        " -1.61551860e+03  2.85237637e+03 -7.97352912e+03 -3.56571820e+03\n",
                        "  4.59929641e+03  7.11323882e+02 -4.24121721e+03 -5.63123455e+03\n",
                        "  6.33170735e+03 -1.20735296e+03 -4.63245604e+00 -1.25499609e+03\n",
                        "  1.11969964e+03  2.87729098e+02 -2.30835658e+03 -1.67513559e+03\n",
                        "  8.87758276e+03 -2.72415182e+03  1.76418950e+04 -2.89504369e+02\n",
                        "  8.97719970e+03 -8.26452464e+03 -9.37393426e+03]\n",
                        "R2 score: 0.9949293788729324\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.365e+09, tolerance: 2.595e+08\n",
                        "  model = cd_fast.enet_coordinate_descent(\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.linear_model import Lasso\n",
                "\n",
                "alpha = 0.9\n",
                "lasso_model = Lasso(alpha = alpha)\n",
                "\n",
                "# Entrenamos el modelo\n",
                "lasso_model.fit(X_train, y_train)\n",
                "\n",
                "# Evaluamos el rendimiento del modelo en los datos de prueba\n",
                "score = lasso_model.score(X_test, y_test)\n",
                "print(\"Coefficients:\", lasso_model.coef_)\n",
                "print(\"R2 score:\", score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pickle import dump\n",
                "\n",
                "dump(lasso_model, open(\"../models/lasso_alpha-0.9.sav\", \"wb\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
